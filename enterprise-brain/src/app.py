import streamlit as st
import os
import glob
import sys
import time
import hashlib
import uuid

# å°† src ç›®å½•åŠ å…¥ Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ ingest æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¼•å…¥å·¥å‚
try:
    from db_factory import DBFactory
    from ingest import ingest_docs
except ImportError:
    from src.db_factory import DBFactory
    from src.ingest import ingest_docs

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="Enterprise Brain",
    page_icon="ğŸ§ ",
    layout="wide"
)

# è·¯å¾„é…ç½®
DB_DIR = "chroma_db"
DATA_DIR = "data"

# ç¡®ä¿ data ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)

@st.cache_resource
def load_chain():
    """
    åˆå§‹åŒ– RAG é“¾ (Embedding + VectorStore + LLM)
    ä½¿ç”¨å·¥å‚æ¨¡å¼è§£è€¦æ•°æ®åº“å®ç°
    """
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # 1. å·¥å‚ç”Ÿäº§å‘é‡åº“ (Chroma æˆ– Pgvectorï¼Œç”± .env å†³å®š)
    vector_store = DBFactory.get_vector_store(embeddings)
    
    # 2. è·å–è¯­ä¹‰ç¼“å­˜ (ç›®å‰å›ºå®šä¸º Chroma)
    cache_collection = DBFactory.get_cache_collection(embeddings)
    
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        temperature=0.1,
        streaming=True
    )
    
    return vector_store, llm, cache_collection, embeddings

def main():
    st.title("ğŸ§  Enterprise Brain (RAG System)")
    st.markdown("---")

    # --- ä¾§è¾¹æ ï¼šçŸ¥è¯†åº“ç®¡ç† ---
    with st.sidebar:
        st.header("ğŸ“‚ Knowledge Base")
        
        # 1. æ–‡ä»¶åˆ—è¡¨
        files = glob.glob(os.path.join(DATA_DIR, "*.* Ø§Ù„Ù…Ø­ØªÙˆÙ‰"))
        if files:
            st.info(f"Loaded {len(files)} documents")
            with st.expander("ğŸ“„ View File List"):
                for f in files:
                    st.text(os.path.basename(f))
        else:
            st.warning("No documents found.")

        st.markdown("---")
        
        # 2. ä¸Šä¼ æ–°æ–‡æ¡£
        st.subheader("ğŸ“¥ Add Documents")
        uploaded_files = st.file_uploader(
            "Upload .md or .txt", 
            type=["md", "txt"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            if st.button("ğŸ’¾ Save & Process"):
                progress_bar = st.progress(0)
                for i, uploaded_file in enumerate(uploaded_files):
                    # ä¿å­˜æ–‡ä»¶
                    save_path = os.path.join(DATA_DIR, uploaded_file.name)
                    with open(save_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                st.success(f"Saved {len(uploaded_files)} files!")
                
                # è§¦å‘é‡å»º
                with st.spinner("ğŸ§  Re-building Brain (Ingesting)..."):
                    ingest_docs()
                    st.cache_resource.clear() # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡è½½å‘é‡åº“
                
                st.success("âœ… Brain Updated Successfully!")
                st.rerun()

        # 3. æ‰‹åŠ¨é‡å»ºæŒ‰é’® (ç”¨äºæ‰‹åŠ¨æ”¾å…¥æ–‡ä»¶å)
        if st.button("ğŸ”„ Re-build Brain (Force)"):
             with st.spinner("ğŸ§  Re-building Brain..."):
                ingest_docs()
                st.cache_resource.clear()
             st.success("Brain reloaded!")
             st.rerun()
            
        st.markdown("---")
        st.caption("Backend: LangChain + DeepSeek + ChromaDB")

    # --- ä¸»èŠå¤©ç•Œé¢ ---
    
    # é¡¶éƒ¨å·¥å…·æ 
    col1, col2 = st.columns([6, 1])
    with col2:
        if st.button("ğŸ—‘ï¸ Clear Chat"):
            st.session_state.messages = []
            st.rerun()

    # 1. åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI æˆ˜ç•¥é¡¾é—®ã€‚åŸºäºä½ ä¸Šä¼ çš„æˆ˜ç•¥æ–‡æ¡£ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"}
        ]

    # 2. æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # 3. å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("Ask a question about your strategy..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”Ÿæˆ AI å›ç­”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # --- RAG é€»è¾‘å¼€å§‹ ---
            try:
                # è·å–ç»„ä»¶
                vector_store, llm, cache_collection, embeddings = load_chain()
                
                # Step 1: è¯­ä¹‰ç¼“å­˜æ£€ç´¢ (Semantic Cache Lookup)
                start_cache = time.time()
                
                # è®¡ç®—é—®é¢˜å‘é‡
                prompt_vector = embeddings.embed_query(prompt)
                
                # åœ¨ç¼“å­˜åº“ä¸­æœæœ€ç›¸ä¼¼çš„ 1 æ¡
                cache_results = cache_collection.query(
                    query_embeddings=[prompt_vector],
                    n_results=1
                )
                
                cache_hit = False
                # åˆ¤å®šé˜ˆå€¼ï¼šChroma é»˜è®¤ä½¿ç”¨ L2 è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼
                # 0.2 å¤§çº¦å¯¹åº”ä½™å¼¦ç›¸ä¼¼åº¦ 0.9+ï¼Œéå¸¸ä¸¥æ ¼ï¼Œé˜²æ­¢ç­”éæ‰€é—®
                CACHE_THRESHOLD = 0.2
                
                if cache_results['ids'] and cache_results['distances'][0][0] < CACHE_THRESHOLD:
                    cached_answer = cache_results['metadatas'][0][0]['answer']
                    message_placeholder.markdown(cached_answer + " (ğŸš€ Semantic Cache)")
                    full_response = cached_answer
                    cache_hit = True
                    
                    st.divider()
                    st.caption(f"âš¡ Semantic Cache Hit (Distance: {cache_results['distances'][0][0]:.4f})")
                
                # Step 2: ç¼“å­˜æœªå‘½ä¸­ï¼Œèµ° RAG
                if not cache_hit:
                    # ... åŸæœ‰ RAG æ£€ç´¢é€»è¾‘ ...
                    start_time = time.time()
                    with st.spinner("ğŸ” Searching..."):
                        results = vector_store.similarity_search(prompt, k=3)
                    retrieval_time = time.time() - start_time
                    
                    if not results:
                        full_response = "âš ï¸ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚"
                        message_placeholder.markdown(full_response)
                    else:
                        context_text = "\n\n---\n\n".join([doc.page_content for doc in results])
                        
                        # ... Prompt æ„å»º ...
                        prompt_template = ChatPromptTemplate.from_template("""
                        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ AI æˆ˜ç•¥é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹çš„ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼Œå›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
                        
                        è§„åˆ™ï¼š
                        1. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®æ•°æ®æˆ–è§‚ç‚¹æ¥æ”¯æŒä½ çš„å›ç­”ã€‚
                        2. ä½¿ç”¨ Markdown æ ¼å¼ä¼˜åŒ–æ’ç‰ˆï¼ˆå¦‚åˆ—è¡¨ã€ç²—ä½“ï¼‰ã€‚
                        3. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚

                        ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼š
                        {context}

                        ã€é—®é¢˜ã€‘ï¼š
                        {question}
                        """)
                        
                        chain = prompt_template | llm
                        
                        start_gen = time.time()
                        full_response = ""
                        for chunk in chain.stream({"context": context_text, "question": prompt}):
                            if chunk.content:
                                full_response += chunk.content
                                message_placeholder.markdown(full_response + "â–Œ")
                        
                        generation_time = time.time() - start_gen
                        message_placeholder.markdown(full_response)
                        
                        # Step 3: å†™å…¥è¯­ä¹‰ç¼“å­˜
                        # ä½¿ç”¨ uuid ä½œä¸º ID
                        cache_id = str(uuid.uuid4())
                        cache_collection.add(
                            ids=[cache_id],
                            embeddings=[prompt_vector], # å¤ç”¨åˆšæ‰ç®—çš„å‘é‡
                            metadatas=[{"answer": full_response, "question": prompt}]
                        )
                        
                        st.divider()
                        cols = st.columns(4)
                        cols[0].caption(f"â±ï¸ Retrieval: **{retrieval_time:.3f}s**")
                        cols[1].caption(f"ğŸ§  Generation: **{generation_time:.3f}s**")
                        cols[2].caption(f"âš¡ Total: **{retrieval_time + generation_time:.3f}s**")
                        
                        # ... Source æ˜¾ç¤º ...
                        with st.expander("ğŸ“š View Sources"):
                            for i, doc in enumerate(results):
                                source = doc.metadata.get('source', 'Unknown')
                                st.markdown(f"**Source {i+1}**: `{os.path.basename(source)}`")
                                st.caption(doc.page_content[:200] + "...")

            except Exception as e:
                full_response = f"âŒ Error: {str(e)}"
                message_placeholder.error(full_response)
            
            # --- RAG é€»è¾‘ç»“æŸ ---

        # ä¿å­˜ AI å›ç­”åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()