import streamlit as st
import os
import sys
import time
import hashlib
import uuid

# å¼•å…¥å·¥åŽ‚
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from db_factory import DBFactory
except ImportError:
    from src.db_factory import DBFactory

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="Enterprise Brain Chat",
    page_icon="ðŸ’¬",
    layout="centered" # èŠå¤©ç•Œé¢é€šå¸¸å±…ä¸­æ›´å¥½çœ‹
)

@st.cache_resource
def load_chain():
    """åˆå§‹åŒ– RAG é“¾"""
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_store = DBFactory.get_vector_store(embeddings)
    cache_collection = DBFactory.get_cache_collection(embeddings)
    
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        temperature=0.1,
        streaming=True
    )
    return vector_store, llm, cache_collection, embeddings

def main():
    st.title("ðŸ’¬ Enterprise Assistant")
    st.caption("ðŸš€ Powered by RAG & DeepSeek")
    
    # é¡¶éƒ¨å·¥å…·æ 
    if st.button("ðŸ—‘ï¸ Clear History", type="secondary"):
        st.session_state.messages = []
        st.rerun()

    # åˆå§‹åŒ–èŠå¤©åŽ†å²
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"}
        ]

    # æ˜¾ç¤ºåŽ†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("Input your question..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            try:
                vector_store, llm, cache_collection, embeddings = load_chain()
                
                # --- Semantic Cache ---
                prompt_vector = embeddings.embed_query(prompt)
                cache_results = cache_collection.query(
                    query_embeddings=[prompt_vector],
                    n_results=1
                )
                
                cache_hit = False
                CACHE_THRESHOLD = 0.2
                
                if (cache_results['ids'] and 
                    cache_results['distances'] and 
                    len(cache_results['distances']) > 0 and 
                    len(cache_results['distances'][0]) > 0 and
                    cache_results['distances'][0][0] < CACHE_THRESHOLD):
                    
                    cached_answer = cache_results['metadatas'][0][0]['answer']
                    message_placeholder.markdown(cached_answer + " (ðŸš€ Cached)")
                    full_response = cached_answer
                    cache_hit = True
                    st.divider()
                    st.caption(f"âš¡ Semantic Cache Hit (Distance: {cache_results['distances'][0][0]:.4f})")
                
                if not cache_hit:
                    # --- RAG ---
                    results = vector_store.similarity_search(prompt, k=3)
                    
                    context_text = ""
                    sources = []
                    if results:
                        context_text = "\n\n---\n\n".join([doc.page_content for doc in results])
                        sources = results
                    
                    prompt_template = ChatPromptTemplate.from_template("""
                    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçº§ AI æˆ˜ç•¥é¡¾é—®ï¼ˆEnterprise Brainï¼‰ã€‚
                    ä½ çš„ä»»åŠ¡æ˜¯åŸºäºŽã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘å›žç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
                    æ ¸å¿ƒè§„åˆ™ï¼š
                    1. å¦‚æžœã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘åŒ…å«ç­”æ¡ˆï¼Œè¯·ç²¾å‡†å¼•ç”¨å¹¶å›žç­”ã€‚
                    2. å¦‚æžœã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ä¸ºç©ºæˆ–ä¸Žé—®é¢˜æ— å…³ï¼ˆä¾‹å¦‚ç”¨æˆ·åœ¨æ‰“æ‹›å‘¼â€œä½ å¥½â€ï¼‰ï¼Œè¯·å¿½ç•¥ä¸Šä¸‹æ–‡ï¼Œç”¨ç¤¼è²Œã€ä¸“ä¸šçš„å£å»è¿›è¡Œè‡ªæˆ‘ä»‹ç»æˆ–é—²èŠã€‚
                    3. è‡ªæˆ‘ä»‹ç»è¯æœ¯å‚è€ƒï¼šâ€œä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI æˆ˜ç•¥é¡¾é—®ã€‚æˆ‘ç†ŸçŸ¥ä½ ä¸Šä¼ çš„æ‰€æœ‰æˆ˜ç•¥æ–‡æ¡£ï¼Œå¯ä»¥å¸®ä½ è§£ç­”å…³äºŽæŠ€æœ¯æž¶æž„ã€å‰¯ä¸šè·¯çº¿ã€SOP æµç¨‹ç­‰é—®é¢˜ã€‚â€
                    
                    ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘:
                    {context}
                    ã€é—®é¢˜ã€‘:
                    {question}
                    """)
                    
                    chain = prompt_template | llm
                    
                    full_response = ""
                    for chunk in chain.stream({"context": context_text, "question": prompt}):
                        if chunk.content:
                            full_response += chunk.content
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                    
                    # å†™å…¥ç¼“å­˜
                    cache_id = str(uuid.uuid4())
                    cache_collection.add(ids=[cache_id], embeddings=[prompt_vector], metadatas=[{"answer": full_response, "question": prompt}])
                    
                    if sources:
                        with st.expander("ðŸ“š Reference"):
                            for i, doc in enumerate(sources):
                                st.caption(f"Source: {os.path.basename(doc.metadata.get('source', 'Unknown'))}")

            except Exception as e:
                full_response = f"Error: {str(e)}"
                message_placeholder.error(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()