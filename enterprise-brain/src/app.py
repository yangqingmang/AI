import streamlit as st
import os
import sys
import time
import hashlib
import uuid

# å¼•å…¥å·¥åŽ‚
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from db_factory import DBFactory
    from tool_factory import ToolFactory
except ImportError:
    from src.db_factory import DBFactory
    from src.tool_factory import ToolFactory

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent
from langchain.tools.retriever import create_retriever_tool
from dotenv import load_dotenv

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="Enterprise Agent",
    page_icon="ðŸ¤–",
    layout="centered"
)

@st.cache_resource
def load_agent(pro_mode=False):
    """
    åˆå§‹åŒ– Agent (ä½¿ç”¨ LangGraph)
    :param pro_mode: æ˜¯å¦å¼€å¯é«˜çº§å·¥å…· (è”ç½‘ã€ä»£ç ã€æ–‡ä»¶)
    """
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_store = DBFactory.get_vector_store(embeddings)
    cache_collection = DBFactory.get_cache_collection(embeddings)
    
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        temperature=0.1,
        streaming=True
    )
    
    # 1. åŸºç¡€å·¥å…· (Free Plan)
    retriever_tool = create_retriever_tool(
        vector_store.as_retriever(search_kwargs={"k": 3}),
        "knowledge_base",
        "æœç´¢ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“ã€‚å…³äºŽå…¬å¸æˆ˜ç•¥ã€SOPã€æŠ€æœ¯æ–‡æ¡£çš„é—®é¢˜ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·ã€‚"
    )
    tools = [retriever_tool]
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçº§ AI æˆ˜ç•¥é¡¾é—®ã€‚
    ä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯åŸºäºŽå†…éƒ¨çŸ¥è¯†åº“å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
    """

    # 2. é«˜çº§å·¥å…· (Pro Plan)
    if pro_mode:
        search_tool = ToolFactory.get_search_tool()
        python_tool = ToolFactory.get_python_tool()
        file_tools = ToolFactory.get_file_tools()
        tools.extend([search_tool, python_tool] + file_tools)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½çš„ä¼ä¸šçº§ AI æ™ºèƒ½ä½“ï¼ˆAutonomous Agentï¼‰ã€‚
        ä½ ä¸ä»…èƒ½å›žç­”é—®é¢˜ï¼Œè¿˜èƒ½ç¼–å†™ä»£ç ã€åˆ†æžæ•°æ®ã€ç®¡ç†æ–‡ä»¶ã€è”ç½‘æœç´¢ã€‚
        """
    
    # 3. ä½¿ç”¨ LangGraph æž„å»º ReAct Agent
    # state_modifier ç›¸å½“äºŽ System Prompt
    agent_executor = create_react_agent(llm, tools, state_modifier=system_prompt)
    
    return agent_executor, cache_collection, embeddings

def main():
    st.title("ðŸ’¬ Enterprise Assistant")
    
    # --- ä¾§è¾¹æ åŠŸèƒ½å¼€å…³ ---
    with st.sidebar:
        st.header("ðŸ’Ž Subscription")
        pro_mode = st.checkbox("Enable Pro Mode (Agent)", value=False, help="Unlock Web Search, Code Execution, and File Management.")
        if pro_mode:
            st.success("ðŸš€ Pro Features Active")
        else:
            st.info("ðŸŒ± Free Plan (RAG Only)")
        st.markdown("---")

    st.caption("ðŸš€ Powered by RAG & LangGraph & DeepSeek")
    
    if st.button("ðŸ—‘ï¸ Clear History", type="secondary"):
        st.session_state.messages = []
        st.rerun()

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"}
        ]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("Input your question..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            try:
                # ä¼ å…¥ pro_mode å¼€å…³çŠ¶æ€
                agent_executor, cache_collection, embeddings = load_agent(pro_mode)
                
                # ... ç¼“å­˜é€»è¾‘ (ä¸å˜) ...
                prompt_vector = embeddings.embed_query(prompt)
                
                cache_results = cache_collection.query(query_embeddings=[prompt_vector], n_results=1)
                
                cache_hit = False
                if (cache_results['ids'] and 
                    cache_results['distances'] and 
                    len(cache_results['distances']) > 0 and 
                    len(cache_results['distances'][0]) > 0 and 
                    cache_results['distances'][0][0] < 0.2):
                    
                    cached_answer = cache_results['metadatas'][0][0]['answer']
                    message_placeholder.markdown(cached_answer + " (ðŸš€ Cached)")
                    full_response = cached_answer
                    cache_hit = True
                
                if not cache_hit:
                    start_time = time.time()
                    with st.status("ðŸ¤– Thinking...", expanded=True) as status:
                        # LangGraph è°ƒç”¨æ–¹å¼: ä¼ å…¥ messages åˆ—è¡¨
                        response = agent_executor.invoke({"messages": [HumanMessage(content=prompt)]})
                        status.update(label="âœ… Finished!", state="complete", expanded=False)
                    
                    # ä»Ž LangGraph è¿”å›žçš„æ¶ˆæ¯åˆ—è¡¨ä¸­æå–æœ€åŽä¸€æ¡ (AIMessage) çš„å†…å®¹
                    full_response = response["messages"][-1].content
                    message_placeholder.markdown(full_response)
                    
                    cache_id = str(uuid.uuid4())
                    cache_collection.add(ids=[cache_id], embeddings=[prompt_vector], metadatas=[{"answer": full_response, "question": prompt}])

            except Exception as e:
                full_response = f"Error: {str(e)}"
                message_placeholder.error(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()