import streamlit as st
import os
import glob
import sys

# å°† src ç›®å½•åŠ å…¥ Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ ingest æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from ingest import ingest_docs
except ImportError:
    # Fallback if running from root
    from src.ingest import ingest_docs

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="Enterprise Brain",
    page_icon="ğŸ§ ",
    layout="wide"
)

# è·¯å¾„é…ç½®
DB_DIR = "chroma_db"
DATA_DIR = "data"

# ç¡®ä¿ data ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)

import chromadb

@st.cache_resource
def load_chain():
    """
    åˆå§‹åŒ– RAG é“¾ (Embedding + VectorStore + LLM)
    ä½¿ç”¨ @st.cache_resource é¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°åŠ è½½æ¨¡å‹
    """
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # åˆ‡æ¢ä¸º HttpClient æ¨¡å¼
    client = chromadb.HttpClient(
        host=os.getenv("CHROMA_SERVER_HOST", "localhost"),
        port=os.getenv("CHROMA_SERVER_PORT", "8000")
    )
    
    vector_store = Chroma(
        client=client,
        collection_name="enterprise_docs",
        embedding_function=embeddings
    )
    
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        temperature=0.1,
        streaming=True # å¼€å¯æµå¼
    )
    
    return vector_store, llm

def main():
    st.title("ğŸ§  Enterprise Brain (RAG System)")
    st.markdown("---")

    # --- ä¾§è¾¹æ ï¼šçŸ¥è¯†åº“ç®¡ç† ---
    with st.sidebar:
        st.header("ğŸ“‚ Knowledge Base")
        
        # 1. æ–‡ä»¶åˆ—è¡¨
        files = glob.glob(os.path.join(DATA_DIR, "*.*"))
        if files:
            st.info(f"Loaded {len(files)} documents")
            with st.expander("ğŸ“„ View File List"):
                for f in files:
                    st.text(os.path.basename(f))
        else:
            st.warning("No documents found.")

        st.markdown("---")
        
        # 2. ä¸Šä¼ æ–°æ–‡æ¡£
        st.subheader("ğŸ“¥ Add Documents")
        uploaded_files = st.file_uploader(
            "Upload .md or .txt", 
            type=["md", "txt"], 
            accept_multiple_files=True
        )
        
        if uploaded_files:
            if st.button("ğŸ’¾ Save & Process"):
                progress_bar = st.progress(0)
                for i, uploaded_file in enumerate(uploaded_files):
                    # ä¿å­˜æ–‡ä»¶
                    save_path = os.path.join(DATA_DIR, uploaded_file.name)
                    with open(save_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                st.success(f"Saved {len(uploaded_files)} files!")
                
                # è§¦å‘é‡å»º
                with st.spinner("ğŸ§  Re-building Brain (Ingesting)..."):
                    ingest_docs()
                    st.cache_resource.clear() # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡è½½å‘é‡åº“
                
                st.success("âœ… Brain Updated Successfully!")
                st.rerun()

        # 3. æ‰‹åŠ¨é‡å»ºæŒ‰é’® (ç”¨äºæ‰‹åŠ¨æ”¾å…¥æ–‡ä»¶å)
        if st.button("ğŸ”„ Re-build Brain (Force)"):
             with st.spinner("ğŸ§  Re-building Brain..."):
                ingest_docs()
                st.cache_resource.clear()
             st.success("Brain reloaded!")
             st.rerun()
            
        st.markdown("---")
        st.caption("Backend: LangChain + DeepSeek + ChromaDB")

    # --- ä¸»èŠå¤©ç•Œé¢ ---
    
    # é¡¶éƒ¨å·¥å…·æ 
    col1, col2 = st.columns([6, 1])
    with col2:
        if st.button("ğŸ—‘ï¸ Clear Chat"):
            st.session_state.messages = []
            st.rerun()

    # 1. åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI æˆ˜ç•¥é¡¾é—®ã€‚åŸºäºä½ ä¸Šä¼ çš„æˆ˜ç•¥æ–‡æ¡£ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"}
        ]

    # 2. æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # 3. å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("Ask a question about your strategy..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

import time

# ... imports ...

# ... existing code ...

        # ç”Ÿæˆ AI å›ç­”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # --- RAG é€»è¾‘å¼€å§‹ ---
            try:
                vector_store, llm = load_chain()
                
                # 1. è®¡æ—¶ï¼šæ£€ç´¢é˜¶æ®µ
                start_time = time.time()
                with st.spinner("ğŸ” Searching..."):
                    results = vector_store.similarity_search(prompt, k=3)
                retrieval_time = time.time() - start_time
                
                if not results:
                    full_response = "âš ï¸ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚"
                    message_placeholder.markdown(full_response)
                else:
                    context_text = "\n\n---\n\n".join([doc.page_content for doc in results])
                    
                    # æ„å»º Prompt
                    prompt_template = ChatPromptTemplate.from_template("""
                    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ AI æˆ˜ç•¥é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹çš„ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼Œå›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
                    
                    è§„åˆ™ï¼š
                    1. å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®æ•°æ®æˆ–è§‚ç‚¹æ¥æ”¯æŒä½ çš„å›ç­”ã€‚
                    2. ä½¿ç”¨ Markdown æ ¼å¼ä¼˜åŒ–æ’ç‰ˆï¼ˆå¦‚åˆ—è¡¨ã€ç²—ä½“ï¼‰ã€‚
                    3. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚

                    ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼š
                    {context}

                    ã€é—®é¢˜ã€‘ï¼š
                    {question}
                    """)
                    
                    chain = prompt_template | llm
                    
                    # 2. è®¡æ—¶ï¼šç”Ÿæˆé˜¶æ®µ
                    start_gen = time.time()
                    full_response = ""
                    for chunk in chain.stream({"context": context_text, "question": prompt}):
                        if chunk.content:
                            full_response += chunk.content
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    generation_time = time.time() - start_gen
                    
                    message_placeholder.markdown(full_response)
                    
                    # 3. æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
                    st.divider()
                    cols = st.columns(4)
                    cols[0].caption(f"â±ï¸ Retrieval: **{retrieval_time:.3f}s**")
                    cols[1].caption(f"ğŸ§  Generation: **{generation_time:.3f}s**")
                    cols[2].caption(f"âš¡ Total: **{retrieval_time + generation_time:.3f}s**")
                    
                    # æ˜¾ç¤ºå¼•ç”¨æ¥æº (Source Expander)
                    with st.expander("ğŸ“š View Sources"):
                        for i, doc in enumerate(results):
                            source = doc.metadata.get('source', 'Unknown')
                            st.markdown(f"**Source {i+1}**: `{os.path.basename(source)}`")
                            st.caption(doc.page_content[:200] + "...")
            except Exception as e:
                full_response = f"âŒ Error: {str(e)}"
                message_placeholder.error(full_response)
            
            # --- RAG é€»è¾‘ç»“æŸ ---

        # ä¿å­˜ AI å›ç­”åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
