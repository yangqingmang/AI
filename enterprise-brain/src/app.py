import streamlit as st
import os
import sys
import time
import hashlib
import uuid

# å¼•å…¥å·¥åŽ‚
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from db_factory import DBFactory
    from tool_factory import ToolFactory
except ImportError:
    from src.db_factory import DBFactory
    from src.tool_factory import ToolFactory

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
from langchain.tools.retriever import create_retriever_tool
from dotenv import load_dotenv

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="Enterprise Agent",
    page_icon="ðŸ¤–",
    layout="centered"
)

@st.cache_resource
def load_agent(pro_mode=False):
    """
    åˆå§‹åŒ– Agent
    :param pro_mode: æ˜¯å¦å¼€å¯é«˜çº§å·¥å…· (è”ç½‘ã€ä»£ç ã€æ–‡ä»¶)
    """
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_store = DBFactory.get_vector_store(embeddings)
    cache_collection = DBFactory.get_cache_collection(embeddings)
    
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL"),
        temperature=0.1,
        streaming=True
    )
    
    # 1. åŸºç¡€å·¥å…· (Free Plan)
    retriever_tool = create_retriever_tool(
        vector_store.as_retriever(search_kwargs={"k": 3}),
        "knowledge_base",
        "æœç´¢ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“ã€‚å…³äºŽå…¬å¸æˆ˜ç•¥ã€SOPã€æŠ€æœ¯æ–‡æ¡£çš„é—®é¢˜ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·ã€‚"
    )
    tools = [retriever_tool]
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçº§ AI æˆ˜ç•¥é¡¾é—®ã€‚
    ä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯åŸºäºŽå†…éƒ¨çŸ¥è¯†åº“å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
    """

    # 2. é«˜çº§å·¥å…· (Pro Plan)
    if pro_mode:
        search_tool = ToolFactory.get_search_tool()
        python_tool = ToolFactory.get_python_tool()
        file_tools = ToolFactory.get_file_tools()
        tools.extend([search_tool, python_tool] + file_tools)
        
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½çš„ä¼ä¸šçº§ AI æ™ºèƒ½ä½“ï¼ˆAutonomous Agentï¼‰ã€‚
        ä½ ä¸ä»…èƒ½å›žç­”é—®é¢˜ï¼Œè¿˜èƒ½ç¼–å†™ä»£ç ã€åˆ†æžæ•°æ®ã€ç®¡ç†æ–‡ä»¶ã€è”ç½‘æœç´¢ã€‚
        """
    
    # 3. å®šä¹‰ ReAct Prompt
    prompt = PromptTemplate.from_template(system_prompt + """
    
    ä½ æœ‰æƒé™è®¿é—®ä»¥ä¸‹å·¥å…·ï¼š
    {tools}
    
    ä½¿ç”¨å·¥å…·çš„æ ¼å¼å¦‚ä¸‹ï¼š
    
    Question: éœ€è¦å›žç­”çš„é—®é¢˜
    Thought: æˆ‘åº”è¯¥æ€Žä¹ˆåšï¼Ÿ
    Action: å·¥å…·åç§° (ä»Ž [{tool_names}] ä¸­é€‰æ‹©)
    Action Input: å·¥å…·çš„è¾“å…¥å†…å®¹
    Observation: å·¥å…·è¿”å›žçš„ç»“æžœ
    ... (Thought/Action/Observation å¯ä»¥é‡å¤å¤šæ¬¡)
    Thought: æˆ‘çŽ°åœ¨çŸ¥é“ç­”æ¡ˆäº†
    Final Answer: æœ€ç»ˆå›žç­”ç»™ç”¨æˆ·çš„ç­”æ¡ˆ
    
    å¼€å§‹ï¼
    
    Question: {input}
    Thought:{agent_scratchpad}
    """)
    
    agent = create_react_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=5
    )
    
    return agent_executor, cache_collection, embeddings

def main():
    st.title("ðŸ’¬ Enterprise Assistant")
    
    # --- ä¾§è¾¹æ åŠŸèƒ½å¼€å…³ ---
    with st.sidebar:
        st.header("ðŸ’Ž Subscription")
        pro_mode = st.checkbox("Enable Pro Mode (Agent)", value=False, help="Unlock Web Search, Code Execution, and File Management.")
        if pro_mode:
            st.success("ðŸš€ Pro Features Active")
        else:
            st.info("ðŸŒ± Free Plan (RAG Only)")
        st.markdown("---")

    st.caption("ðŸš€ Powered by RAG & DeepSeek")
    
    if st.button("ðŸ—‘ï¸ Clear History", type="secondary"):
        st.session_state.messages = []
        st.rerun()

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"}
        ]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("Input your question..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            try:
                # ä¼ å…¥ pro_mode å¼€å…³çŠ¶æ€
                agent_executor, cache_collection, embeddings = load_agent(pro_mode)
                
                # ... ç¼“å­˜é€»è¾‘ (ä¸å˜) ...
                prompt_vector = embeddings.embed_query(prompt)
                # æ³¨æ„ï¼šä¸ºäº†ç®€å•æ¼”ç¤ºï¼Œè¿™é‡Œç¼“å­˜æ²¡æœ‰åŒºåˆ† Pro/Freeã€‚
                # ç”Ÿäº§çŽ¯å¢ƒå»ºè®® cache_key åŠ ä¸Š pro_mode å‰ç¼€ï¼Œé˜²æ­¢ Free ç”¨æˆ·è¯»åˆ° Pro ç”Ÿæˆçš„é«˜çº§ç­”æ¡ˆï¼ˆæˆ–è€…åä¹‹ï¼‰ã€‚
                cache_results = cache_collection.query(query_embeddings=[prompt_vector], n_results=1)
                
                cache_hit = False
                if (cache_results['ids'] and 
                    cache_results['distances'] and 
                    len(cache_results['distances']) > 0 and 
                    len(cache_results['distances'][0]) > 0 and 
                    cache_results['distances'][0][0] < 0.2):
                    
                    cached_answer = cache_results['metadatas'][0][0]['answer']
                    message_placeholder.markdown(cached_answer + " (ðŸš€ Cached)")
                    full_response = cached_answer
                    cache_hit = True
                
                if not cache_hit:
                    start_time = time.time()
                    with st.status("ðŸ¤– Thinking...", expanded=True) as status:
                        result = agent_executor.invoke({"input": prompt})
                        status.update(label="âœ… Finished!", state="complete", expanded=False)
                    
                    full_response = result["output"]
                    message_placeholder.markdown(full_response)
                    
                    cache_id = str(uuid.uuid4())
                    cache_collection.add(ids=[cache_id], embeddings=[prompt_vector], metadatas=[{"answer": full_response, "question": prompt}])

            except Exception as e:
                full_response = f"Error: {str(e)}"
                message_placeholder.error(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
