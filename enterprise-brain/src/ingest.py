import os
import glob
import hashlib
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv
import chromadb

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

DATA_DIR = "data"
COLLECTION_NAME = "enterprise_docs"

def calculate_file_hash(filepath):
    """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
    hasher = hashlib.md5()
    try:
        with open(filepath, 'rb') as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()
    except Exception as e:
        print(f"âš ï¸ Error reading {filepath}: {e}")
        return ""

def ingest_docs():
    """
    å…¨é‡åŒæ­¥ data/ ç›®å½•åˆ° ChromaDBï¼š
    1. æ–°å¢ï¼šæ–°æ–‡ä»¶å…¥åº“
    2. æ›´æ–°ï¼šæ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œé‡æ–°å…¥åº“ (åˆ æ—§å¢æ–°)
    3. åˆ é™¤ï¼šæ–‡ä»¶è¢«åˆ ï¼Œåº“ä¸­ä¹Ÿåˆ é™¤
    """
    print("ğŸ”Œ Connecting to ChromaDB Server...")
    try:
        client = chromadb.HttpClient(
            host=os.getenv("CHROMA_SERVER_HOST", "localhost"),
            port=os.getenv("CHROMA_SERVER_PORT", "8000")
        )
        collection = client.get_or_create_collection(name=COLLECTION_NAME)
    except Exception as e:
        print(f"âŒ Could not connect to ChromaDB: {e}")
        return

    # --- 1. è·å–æ•°æ®åº“ç°æœ‰çŠ¶æ€ ---
    # è·å–æ‰€æœ‰è®°å½•çš„ source å’Œ file_hash å…ƒæ•°æ®
    print("ğŸ” Scanning database state...")
    existing_data = collection.get(include=["metadatas"])
    
    # å»ºç«‹æ˜ å°„: source_path -> {ids: [id1, id2...], hash: "abc..."}
    db_state = {} 
    
    if existing_data and existing_data["ids"]:
        for i, doc_id in enumerate(existing_data["ids"]):
            meta = existing_data["metadatas"][i]
            if not meta: continue
            
            source = meta.get("source")
            if not source: continue
            
            # ç»Ÿä¸€è·¯å¾„æ ¼å¼ä»¥ä¾¿æ¯”è¾ƒ
            norm_source = os.path.normpath(source)
            
            if norm_source not in db_state:
                db_state[norm_source] = {"ids": [], "hash": meta.get("file_hash", "")}
            db_state[norm_source]["ids"].append(doc_id)

    print(f"ğŸ‘€ DB contains chunks from {len(db_state)} files.")

    # --- 2. è·å–æœ¬åœ°æ–‡ä»¶çŠ¶æ€ ---
    local_files = glob.glob(os.path.join(DATA_DIR, "**/*.md"), recursive=True) +
                  glob.glob(os.path.join(DATA_DIR, "**/*.txt"), recursive=True)
    
    local_state = {} # path -> hash
    for f in local_files:
        norm_path = os.path.normpath(f)
        local_state[norm_path] = calculate_file_hash(norm_path)

    print(f"ğŸ“‚ Local folder contains {len(local_state)} files.")

    # --- 3. è®¡ç®—å·®å¼‚ ---
    to_add = []      # (path, hash)
    to_update = []   # (path, hash, old_ids)
    to_delete = []   # (path, all_ids)

    # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶ (æ–°å¢æˆ–æ›´æ–°)
    for f_path, f_hash in local_state.items():
        if f_path not in db_state:
            # æ–°å¢
            to_add.append((f_path, f_hash))
        else:
            # æ£€æŸ¥æ˜¯å¦ä¿®æ”¹
            db_info = db_state[f_path]
            # å¦‚æœæ•°æ®åº“é‡Œçš„ hash æ˜¯ç©ºçš„ï¼ˆæ—§æ•°æ®ï¼‰ï¼Œæˆ–è€… hash ä¸ä¸€è‡´ï¼Œéƒ½è§†ä¸ºæ›´æ–°
            if db_info["hash"] != f_hash:
                to_update.append((f_path, f_hash, db_info["ids"]))

    # æ£€æŸ¥å·²åˆ é™¤æ–‡ä»¶
    for db_path, info in db_state.items():
        if db_path not in local_state:
            to_delete.append((db_path, info["ids"]))

    print(f"ğŸ“Š Sync Plan: +Add {len(to_add)} | ~Update {len(to_update)} | -Delete {len(to_delete)}")

    if not to_add and not to_update and not to_delete:
        print("âœ… Everything is up to date.")
        return

    # --- 4. æ‰§è¡ŒåŒæ­¥ ---
    
    # 4.1 åˆ é™¤æ“ä½œ (Delete & Update-Delete)
    ids_to_remove = []
    for item in to_delete:
        print(f"   ğŸ—‘ï¸  Marked for deletion: {os.path.basename(item[0])}")
        ids_to_remove.extend(item[1])
    
    for item in to_update:
        print(f"   ğŸ”„ Marked for update: {os.path.basename(item[0])}")
        ids_to_remove.extend(item[2])
        
    if ids_to_remove:
        # æ‰¹é‡åˆ é™¤ï¼Œé¿å…è¯·æ±‚è¿‡å¤§ï¼Œåˆ†æ‰¹åˆ ï¼ˆè™½ç„¶ Chroma æ”¯æŒè¾ƒå¤§ batchï¼Œä½†ç¨³å¦¥èµ·è§ï¼‰
        batch_size = 5000 
        for i in range(0, len(ids_to_remove), batch_size):
            batch = ids_to_remove[i:i+batch_size]
            print(f"   ğŸ”¥ Deleting batch of {len(batch)} chunks...")
            collection.delete(ids=batch)

    # 4.2 æ–°å¢/é‡æ–°æ’å…¥æ“ä½œ (Add & Update-Add)
    files_to_process = [x[0] for x in to_add] + [x[0] for x in to_update]
    
    if not files_to_process:
        print("âœ… Sync complete (Only deletions performed).")
        return

    # åŠ è½½å¹¶åˆ‡åˆ†
    documents = []
    for f in files_to_process:
        try:
            # åŸå§‹è·¯å¾„å¯èƒ½åœ¨ local_state çš„ key é‡Œè¢« normpath äº†ï¼Œè¿™é‡Œå°½é‡ç”¨åŸå§‹globå‡ºæ¥çš„è·¯å¾„æˆ–è¿˜åŸ
            # ä½† TextLoader åªè¦è·¯å¾„å­˜åœ¨å³å¯ã€‚f æ˜¯ normpath è¿‡çš„ã€‚
            loader = TextLoader(f, encoding='utf-8')
            docs = loader.load()
            
            current_hash = local_state[f]

            for doc in docs:
                doc.metadata["source"] = f
                doc.metadata["filename"] = os.path.basename(f)
                doc.metadata["file_hash"] = current_hash # å…³é”®ï¼šå­˜å…¥ Hash
            documents.extend(docs)
            print(f"   - Loaded: {os.path.basename(f)}")
        except Exception as e:
            print(f"   âŒ Failed to load {f}: {e}")

    if documents:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)
        
        print("ğŸ§  Initializing embedding model (HuggingFace)...")
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        vector_store = Chroma(
            client=client,
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings
        )
        
        # åˆ†æ‰¹æ’å…¥ä»¥é˜²å†…å­˜æº¢å‡ºæˆ–è¶…æ—¶
        batch_size = 100
        total_chunks = len(chunks)
        print(f"ğŸ’¾ Ingesting {total_chunks} chunks...")
        
        for i in range(0, total_chunks, batch_size):
            batch = chunks[i:i+batch_size]
            vector_store.add_documents(batch)
            print(f"      ...ingested {min(i+batch_size, total_chunks)}/{total_chunks}")

    print("âœ… Sync complete!")

if __name__ == "__main__":
    ingest_docs()
