import os
import glob
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

# åŠ è½½ .env
load_dotenv()

# é…ç½®è·¯å¾„
DATA_DIR = "data"
DB_DIR = "chroma_db"

import chromadb

def ingest_docs():
    print(f"ğŸ“‚ Loading documents from {DATA_DIR}...")
    
    documents = []
    for f in files:
        try:
            loader = TextLoader(f, encoding='utf-8')
            docs = loader.load()
            # ä¸°å¯Œå…ƒæ•°æ®ï¼šæ·»åŠ æ–‡ä»¶åã€æ–‡ä»¶è·¯å¾„
            for doc in docs:
                doc.metadata["source"] = f
                doc.metadata["filename"] = os.path.basename(f)
            documents.extend(docs)
            print(f"   - Loaded: {f}")
        except Exception as e:
            print(f"   âŒ Failed to load {f}: {e}")

    # 3. åˆå§‹åŒ– Embedding æ¨¡å‹
    print("ğŸ§  Initializing embedding model (HuggingFace)...")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    # 4. è¿æ¥æœåŠ¡å™¨å¹¶å…¥åº“ (Client/Server æ¨¡å¼)
    print(f"ğŸ’¾ Sending vectors to Chroma Server...")
    
    client = chromadb.HttpClient(
        host=os.getenv("CHROMA_SERVER_HOST", "localhost"),
        port=os.getenv("CHROMA_SERVER_PORT", "8000")
    )

    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        client=client,
        collection_name="enterprise_docs"
    )
    
    print("âœ… Ingestion complete! The Brain (Server) is ready.")

if __name__ == "__main__":
    ingest_docs()
